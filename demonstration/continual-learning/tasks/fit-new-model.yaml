apiVersion: tekton.dev/v1beta1
kind: Task
metadata:
  name: fit-model-stateless
  labels:
    app.kubernetes.io/version: "0.6"
  annotations:
    tekton.dev/pipelines.minVersion: "0.17.0"
    tekton.dev/categories: Image Build
    tekton.dev/tags: image-build
    tekton.dev/displayName: "Get historical feature values from offline store"
    tekton.dev/platforms: "linux/amd64"
spec:
  description: >-
    This Task fits a fresh xgb model on input data.
  params:
    - name: mlflow_model_name
      description: Name of MLFlow model to load.
    - name: mlflow_tracking_uri
      description: Reachable MLFlow Endpoint.
    - name: mlflow_experiment_name
      description: Name of MLFlow experiment.
  workspaces:
    - name: shared-workspace
      description: This workspace contains model parameters from MLFlow.
  results:
    - name: storage-uri
      description: The uri of the stored model in s3.
  steps:
    - name: load-and-fit
      image: docker.io/piepra/ml-executor:1.6
      script: | 
        #!/usr/bin/env bash
        pip install matplotlib
        pyscript=$(cat <<'EOF'   
        import pickle
        import os
        import argparse
        import logging
        import numpy as np
        import pandas as pd
        import mlflow
        from mlflow.models.signature import infer_signature
        from xgboost import XGBRegressor
        from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
        from sklearn.model_selection import GridSearchCV
        from mlflow.models.signature import infer_signature
        from sklearn.model_selection import train_test_split

        parser = argparse.ArgumentParser(description='Load and Fit Model.')
        parser.add_argument('--mlflow_experiment_name', help='The name of the MLFlow experiment.', type=str, required = True)
        parser.add_argument('--mlflow_tracking_uri', help='Reachable MLFlow URI.', type=str, required = True)
        parser.add_argument('--mlflow_model_name', help='Name of the MLFlow model.', type=str, required = True)
        parser.add_argument('--random_state', help='The random seed.', type=str, default = "42")
        parser.add_argument('--destination', help='The pickle file destination', type=str, default = ".")
        args = parser.parse_args()

        os.environ["AWS_ACCESS_KEY_ID"] = "mlflow"
        os.environ["AWS_SECRET_ACCESS_KEY"] = "mlflow123"
        os.environ["MLFLOW_S3_ENDPOINT_URL"] = f"http://mlflow-minio.mlflow.svc.cluster.local:9000/"

        mlflow.set_tracking_uri(args.mlflow_tracking_uri)
        mlflow.set_experiment(args.mlflow_experiment_name)
        df = pd.read_csv(args.destination + "/features.csv", index_col="event_timestamp")
        df.index = pd.to_datetime(df.index)
        first = df.index[0].timestamp()
        last = df.index[-1].timestamp()
        keys = list(df.columns)
        keys.remove("y")
        X = df[keys]
        y = df["y"]

        random_state=int(args.random_state)
        train_size = 0.9
        X = df[['open_btc', 'high_btc', 'low_btc', 'close_btc', 'open_eth', 'high_eth', 'low_eth', 'close_eth']]
        y = df['y']

        X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=train_size, random_state=random_state)

        def eval_metrics(actual, pred):
            rmse = np.sqrt(mean_squared_error(actual, pred))
            mae = mean_absolute_error(actual, pred)
            r2 = r2_score(actual, pred)
            return rmse, mae, r2

        parameters = {
            'n_estimators': [600, 800, 1000],
            'learning_rate': [0.01, 0.2],
            'max_depth': [2, 3, 4],
            'gamma': [0.0001, 0.001],
            'random_state': [42]
        }
        model = XGBRegressor(objective='reg:squarederror')
        clf = GridSearchCV(model, parameters, n_jobs=-1)
        clf.fit(X_train, y_train)


        with mlflow.start_run():
            mlflow.xgboost.autolog()
            model = XGBRegressor(**clf.best_params_, objective='reg:squarederror')
            model.fit(X_train, y_train, verbose=False)
            logging.warning(clf.best_params_)
            # Evaluate the best model with testing data.
            y_hat = model.predict(X_test)
            (rmse, mae, r2) = eval_metrics(y_test, y_hat)
            mlflow.log_param("data_from", first)
            mlflow.log_param("data_to", last)
            mlflow.log_param("feature_view", "crypto_stats")
            mlflow.log_param("framework", "xgboost")
            mlflow.log_param("random_state", random_state)
            mlflow.log_param("num_items", len(df))
            mlflow.log_metric("rmse", rmse)
            mlflow.log_metric("r2", r2)
            mlflow.log_metric("mae", mae)
            model_signature = infer_signature(X_train, y_train)
            info = mlflow.xgboost.log_model(model, "model", registered_model_name="BitcoinForecast", signature=model_signature)
            uri = mlflow.get_artifact_uri()
            storage_uri = f"{uri}/model"
            with open("$(results.storage-uri.path)", 'a') as f: f.write(storage_uri)
        EOF
        )
        python -c "$pyscript" --mlflow_tracking_uri $(params.mlflow_tracking_uri) --mlflow_model_name $(params.mlflow_model_name) \
        --mlflow_experiment_name $(params.mlflow_experiment_name) --destination $(workspaces.shared-workspace.path)